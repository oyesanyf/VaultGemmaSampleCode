# Core ML and NLP libraries
# Note: If you install Transformers from Hugging Face wheels, keep the extra-index below.
# For CUDA 12.1 builds, uncomment the cu121 line; for CPU-only, you can use the cpu line.
torch>=2.0.0,<2.5.0
# Install latest Transformers from source to support vaultgemma
git+https://github.com/huggingface/transformers.git
protobuf>=3.20.3,<5.0.0
sentencepiece>=0.1.99,<0.2.0
tiktoken>=0.5.0,<1.0.0
datasets>=2.14.0,<3.0.0
accelerate>=0.20.0,<1.0.0

# PEFT (Parameter Efficient Fine-Tuning)
peft>=0.8.0,<1.0.0

# Differential Privacy
opacus>=1.5.0,<2.0.0

# Data processing and utilities
pandas>=1.5.0,<3.0.0
numpy>=1.21.0,<2.0.0
faker>=19.0.0,<25.0.0

# Progress bars and CLI
tqdm>=4.64.0,<5.0.0

# Optional: Secure random number generation for DP
torchcsprng>=0.2.0,<1.0.0; extra == "secure"

# Development dependencies (optional)
pytest>=7.0.0,<8.0.0; extra == "dev"
black>=23.0.0,<25.0.0; extra == "dev"
flake8>=6.0.0,<8.0.0; extra == "dev"
mypy>=1.0.0,<2.0.0; extra == "dev"
